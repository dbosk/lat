\documentclass[a4paper,article,oneside]{memoir}
\let\subsubsection\subsection
\let\subsection\section
\let\section\chapter

\input{preamble.tex}
\usepackage{didactic}

\title{Some stats for Canvas courses}
\author{Daniel Bosk}

\begin{document}
\maketitle
\tableofcontents
@

\begin{pycode}
import os
import subprocess

vars = {
  "RESULTS_CACHE": ".",
  "tilkry24_LAB1": "canvaslms.grades.tilkryLAB1",
  "tilkry25_LAB1": "canvaslms.grades.tilkryLAB1",
  "prgi22_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgm22_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgi23_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgm23_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgi24_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgm24_LAB2": "canvaslms.grades.maxgradesurvey"
}

def minted_output(command, shell=False):
  env = os.environ.copy()
  env.update(vars)

  output = subprocess.run(command, capture_output=True,
                          shell=shell, env=env)

  print(r'\begin{minted}{text}')
  stdout = output.stdout.decode('utf-8').strip()
  if stdout:
    print(stdout)
  stderr = output.stderr.decode('utf-8').strip()
  if stderr:
    print(stderr)
  print(r'\end{minted}')
\end{pycode}

\section{Some stats from the tilkry course}

We'll give an overview of the stats here.
We'll use the tilkry24 course as an example.
In the following section we see how the results are computed.
In this section we'll just run the script and show the output here.

\subsection{Results on INL1}

The INL1 module has only mandatory assignments.
Let's start with the overall grades,
[[results grades tilkry24 INL1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'INL1'])
\end{pycode}

We can see that there are some Fs.
Let's see which assignments generated Fs,
[[results fails tilkry24 INL1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'INL1'])
\end{pycode}
Most are due to the quiz.

On tilkry24 they only had one chance on the INL1Quiz, but on tilkry25 they got 
to retry until they passed.
We want to see how many attempts the students made to pass the quiz.
[[results attempts tilkry25 INL1]] yields:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'tilkry25', 'INL1'])
\end{pycode}
We can compare that to the results on tilkry24,
[[results attempts tilkry24 INL1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'tilkry24', 'INL1'])
\end{pycode}

\subsection{Results on LAB1}

Let's move on to LAB1.
This is the more complex part where there are optional assignments, and 
depending on how the student does they'll get a grade in the scale between A 
and F.
We'll start with the overall grades,
[[results grades tilkry24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'LAB1'])
\end{pycode}
Most students did well.
Particularly many got an A or B.

Let's see which assignments generated Fs,
[[results fails tilkry24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'LAB1'])
\end{pycode}
Only one.
This is probably due to the students not having submitted some of the mandatory 
assignments.
Therefore, let's have a look at which assignments missed the most submissions,
[[results missing tilkry24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'missing', 'tilkry24', 'LAB1'])
\end{pycode}
We see that rather many of the mandatory assignments are missing submissions.

To tilkry25 we made the presentation of AES mandatory.
This forced the students to pick one more lab for a higher grade.
This changed the distribution of missing submissions.
[[results missing tilkry25 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'missing', 'tilkry25', 'LAB1'])
\end{pycode}

\section{Some stats from the prgm and prgi courses}

We'll now test the script on another course.
We'll use it on the LAB3 module of prgm23 and prgi23:
[[results all prg[im]23 LAB3]].
\begin{pycode}
minted_output(['bash', 'results.sh', 'all', 'prg[im]23', 'LAB3'])
\end{pycode}
We see that we can get the results for both courses taken together.
But let's compare them.

\subsection{Results on LAB1}

We want to compare the results on LAB1 for prgm23, prgm24, prgi23 and prgi24.
So we can run [[all]] on each one separately and then compare the results.
However, it's nicer to do each individually so that we can compare more easily.

Let's start with the grades.
[[results grades prgm23 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm23', 'LAB1'])
\end{pycode}
[[results grades prgm24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm24', 'LAB1'])
\end{pycode}
[[results grades prgi23 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi23', 'LAB1'])
\end{pycode}
[[results grades prgi24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi24', 'LAB1'])
\end{pycode}

Next we want to see the number of attempts.
[[results attempts prgm23 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm23', 'LAB1'])
\end{pycode}
[[results attempts prgm24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm24', 'LAB1'])
\end{pycode}
[[results attempts prgi23 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi23', 'LAB1'])
\end{pycode}
[[results attempts prgi24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi24', 'LAB1'])
\end{pycode}


\subsection{Results on LAB2}

We also want to compare the results on LAB2 for prgm23, prgm24, prgi23 and 
prgi24.
But here we want to see the number of attempts of those who passed\footnote{%
  Due to the construction of the Datorprov setup, we must run the attempts on 
  another assignment group than LAB2.
  Because LAB2 only contains the final grade, which doesn't account for the 
  number of attempts.
} and how many failed.
Let's start with the grade distributions.
[[results grades prgm23 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm23', 'LAB2'])
\end{pycode}
[[results grades prgm24 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi23', 'LAB2'])
\end{pycode}
[[results grades prgi23 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm24', 'LAB2'])
\end{pycode}
[[results grades prgi24 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi24', 'LAB2'])
\end{pycode}

Now, let's have a look at the number of attempts.
Here we can't use LAB2, because the submissions are not done in any assignment 
there.
The assignments are distributed as follows.
\begin{verbatim}
DD1317 HT23 (prgi23)	LAB2	Datorprov
DD1317 HT24 (prgi24)	LAB2	Betyg datorprovet
DD1317 HT24 (prgi24)	Datorprov	Exempelprov
DD1317 HT24 (prgi24)	Datorprov	Datorprov (v2024)
DD1317 HT24 (prgi24)	Datorprov	Inledande diagnostiskt prov
DD1310 HT23 (prgm23)	LAB2	Betyg datorprovet
DD1310 HT23 (prgm23)	Imported Assignments	Datorprov (oktober)
DD1310 HT23 (prgm23)	Imported Assignments	Datorprov (december och augusti)
DD1310 HT24 (prgm24)	Bonus	Ackumulerad datorprovspo√§ng
DD1310 HT24 (prgm24)	LAB2	Betyg datorprovet
DD1310 HT24 (prgm24)	Datorprov	Datorprov
DD1310 HT24 (prgm24)	Datorprov	Exempelprov
DD1310 HT24 (prgm24)	Datorprov	Inledande diagnostiskt prov
\end{verbatim}
We see that for prgm23, prgm24, prgi24 the \enquote{Datorprov} assignment is 
not in LAB2, only \enquote{Betyg datorprovet} is.
But the attempts are in the \enquote{Datorprov} assignment, which is in the 
\enquote{Datorprov} assignment group.
(A script transfers the results from the \enquote{Datorprov} assignment to the 
\enquote{Betyg datorprovet} assignment, but Canvas only counts that as changing 
the grade of \emph{one} submission.)
However, for prgi23, the \enquote{Datorprov} assignment is in LAB2.
It was the same for prgm22.

Let's turn to the results.
Let's see how the number of attempts have changed over the years.
[[results attempts prgm22 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm22', 'LAB2'])
\end{pycode}
[[results attempts prgi22 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi22', 'LAB2'])
\end{pycode}
So far, prgm22 and prgi22 did the same test.
However, for prgm23 we changed the test.
Now they only had two chances, but could earn bonus points.
[[results attempts prgm23 Datorprov]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm24', 'Datorprov'])
\end{pycode}
But we kept the old test for prgi23.
[[results attempts prgi23 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi23', 'LAB2'])
\end{pycode}
[[results attempts prgm24 Datorprov]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm24', 'Datorprov'])
\end{pycode}
For prgi24 we changed the test to the same as for prgm23 and prgm24.
However, they didn't have the exam proctored.
They also couldn't earn bonus points for the test.
But they could take the test once a month throughout the academic year.
[[results attempts prgi24 Datorprov]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi24', 'Datorprov'])
\end{pycode}
For prgi25, the test will be proctored, same as for prgm23, prgm24, prgm25.

\subsection{Results on LAB3}

We also want to compare the results on LAB3 for prgm23, prgm24, prgi23 and 
prgi24.
Let's start with the grades.
[[results grades prgm23 LAB3]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm23', 'LAB3'])
\end{pycode}
[[results grades prgi23 LAB3]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi23', 'LAB3'])
\end{pycode}
[[results grades prgm24 LAB3]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm24', 'LAB3'])
\end{pycode}
[[results grades prgi24 LAB3]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi24', 'LAB3'])
\end{pycode}

Next we want to see the number of attempts.
[[results attempts prgm23 LAB3]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm23', 'LAB3'])
\end{pycode}
[[results attempts prgi23 LAB3]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi23', 'LAB3'])
\end{pycode}
[[results attempts prgm24 LAB3]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm24', 'LAB3'])
\end{pycode}
[[results attempts prgi24 LAB3]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi24', 'LAB3'])
\end{pycode}



\section{The script, or, how the results are made}

We'll write a short shell script that fetches the results from Canvas and 
summarizes them in statistics.
This will be done in the same way as they are reported to LADOK, so they'll be 
representative.
<<results.sh>>=
#!/bin/bash

<<constants>>
<<functions>>

main() {
  <<call specified function>>
}

# call main if not sourced
# https://stackoverflow.com/a/28776166/1305099
(return 0 2>/dev/null) || main "$@"
@

We'll also make calls to that code in this documentation, to get the results 
and how they were generated.
<<Makefile>>=
<<Makefile variables>>

.PHONY: all
all: results.pdf results

results.pdf: results.tex results.sh
results: results.sh
	ln -f $< $@
	chmod +x $@
@

We'll also build this Makefile.
We're fortunately enough that [[make]] tries to remake any Makefile that it 
loads.
<<Makefile>>=
Makefile: results.nw
	${NOTANGLE.mk}
@

We'll execute the script in the documentation using PythonTeX, so we need the 
[[-shell-escape]] flag.
<<Makefile variables>>=
LATEXFLAGS+=  -shell-escape
@

We'll also use the [[didactic]] package.
We'll use the one from the repo instead of an installed version.
<<Makefile>>=
results.pdf: didactic.sty

INCLUDE_DIDACTIC=../didactic
include ${INCLUDE_DIDACTIC}/didactic.mk
@

Finally, we'll use some automation to build.
<<Makefile>>=
INCLUDE_MAKEFILES=../makefiles
include ${INCLUDE_MAKEFILES}/tex.mk
include ${INCLUDE_MAKEFILES}/noweb.mk
@


\subsection{Calling the functions}

We'll take the function name as the first argument and simply call it.
If we don't have any arguments, we'll print a help text.
<<call specified function>>=
[[ "$1" = "" ]] \
  && <<echo help text>> \
  || "$@"
<<echo help text>>=
echo -e "Usage: $0 [function]\nFunctions:\n<<list all functions>>"
<<list all functions>>=
$(sed -En 's/^(\w+)\(\) \{/  \1/p' $0 | grep -Pv '^\s*main$')
@

We'll also add a function that runs all the most interesting functions.
<<functions>>=
all() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    echo
    <<call all functions>>
  done
}
@


\section{Implementation of the stats}

Now we'll go through the implementation of the stats functions.

\subsection{Fetching the results}

We'll now defined two functions that can be used to fetch the desired results.
These will also act as a cache, so that we don't fetch results more than once.

The functions will take two arguments, the course and the module.
On tilkry24 we have two modules: LAB1 and INL1.

LAB1 needs a particular summary module to compute the final grade ([[-S]] 
option to [[canvaslms results]]).
So we need to treat that with a special case.
But INL1 can use the default.

We'll use a function to fetch the results for each module.
It takes the course and module as an argument.

We'll compute the grades using [[canvaslms results]], which is the same way as 
the results are reported to LADOK.
However, this takes time, so we'll only do it if the file doesn't already 
exist.
This way each function can call the same function to fetch the results, but it 
will slow down only if they don't already exist.
<<functions>>=
fetch_results() {
  local course="$1"
  local module="$2"
  local file="${cache_dir}/results-${course}-${module}.csv"

  echo "$file"
  [[ -f "$file" ]] && return

  <<variables for summary module>>

  canvaslms results \
    -c "${course}" -F '' \
    -A "^${module}" \
    <<add [[-S]] option if needed>> \
    > "${file}"
}
@

We want to cache the results, so we'll set a cache directory.
This way we can control where the cache is stored.
For instance, if we set it to [[/tmp]], the cache will be removed when the 
system is rebooted or a sufficiently long time has passed.
But we might just as well set it to [[.]] to keep the cache in the current 
working directory.

We will also let the user set the cache directory by setting an environment 
variable.
If that is set we'll use it, otherwise we'll use [[/tmp]] by default.
<<constants>>=
cache_dir="${RESULTS_CACHE:-/tmp}"
@

If the module needs a summary module, we'll handle it by setting a particular 
environment variable.
As mentioned above, tilkry24 LAB1 needed a special summary module.
If we set
\begin{center}
  [[tilkry24_LAB1=canvaslms.grades.tilkryLAB1]]
\end{center}
the function will use it.
<<variables for summary module>>=
local summary_module=`get_summary_module "${course}" "${module}" 2> /dev/null`
<<functions>>=
get_summary_module() {
  local course="$1"
  local module="$2"
  local var_name="${course}_${module}"
  echo "${!var_name}"
}
<<add [[-S]] option if needed>>=
$([[ -n "$summary_module" ]] && echo "-S $summary_module")
@

We'll do the same for fetching submission results for the individual 
assignments (not the module as a whole).
Unlike the previous one, there is no special case here.
<<functions>>=
fetch_submissions() {
  local course="$1"
  local module="$2"
  local file="${cache_dir}/submissions-${course}-${module}.csv"

  echo "$file"
  [[ -f "$file" ]] && return

  canvaslms submissions \
    -c "${course}" \
    -A "^${module}" \
    > "${file}"
}
@

We want to construct [[fetch_history]] similarly to [[fetch_submissions]], with 
a cache.
The only difference to the others is that we use the [[-H]] option to get the 
history of submissions instead of only the latest.
<<functions>>=
fetch_history() {
  local course="$1"
  local module="$2"
  local file="${cache_dir}/history-${course}-${module}.csv"

  echo "$file"
  [[ -f "$file" ]] && return

  canvaslms submissions \
    -c "${course}" \
    -A "^${module}" \
    -H \
    > "${file}"
}
@

Finally, we also want to be able to clear the results cache.
We call this function to refetch the results, to sound less dramatic\footnote{%
  Clean and clear might sound like we're deleting the results, but we're just 
  refetching them.
}.
<<functions>>=
refetch() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    rm -f "${cache_dir}/results-${course}-${module}.csv"
    rm -f "${cache_dir}/submissions-${course}-${module}.csv"
    rm -f "${cache_dir}/history-${course}-${module}.csv"
  done
}
@

Now let's look at the stats.
We'll add one function per stat we want.

\subsection{Grades of modules}

We'll start by looking at the grades of the modules.
We'll create a function that generates stats for a module, where the module is 
passed as an argument.
<<functions>>=
grades() {
  local course="$1"
  shift
  local modules="$@"
  
  for module in $modules; do
    local results=`fetch_results "$course" "$module"`
    echo
    echo "${course} ${module} grades:"
    cat "$results" | cut -f 4 | sort | uniq -c
  done
}
@

If we consider running [[grades tilkry24 LAB1 INL1]], the result looks like 
this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'LAB1', 'INL1'])
\end{pycode}

This is useful enough to add to the [[all]] function.
<<call all functions>>=
echo
grades "$course" "$module"
@

\subsection{Which assignments generated Fs?}\label{GeneratedFs}

We want a function that takes the course and a module as an argument and then 
outputs the distribution of Fs for the assignments.
We'll do this by filtering out the Fs and then count the occurrences of each 
assignment.
<<functions>>=
fails() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    local submissions=`fetch_submissions "$course" "$module"`
    echo
    echo "${course} ${module} Fs:"
    grep <<pattern for failed grades>> "$submissions" \
      | cut -f 2 | sort | uniq -c \
      | sort -n
  done
}
<<pattern for failed grades>>=
-E "\\s($FAILED_PATTERN)\\s"
<<constants>>=
FAILED_PATTERN='Fx?|incomplete'
@

If we consider running [[fails tilkry24 INL1]], the result looks like this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'INL1'])
\end{pycode}

This is also useful enough to add to the [[all]] function.
<<call all functions>>=
echo
fails "$course" "$module"
@

\subsection{Which assignments were the least popular?}

We now want to see which assignments the students did and which they did not.
In \cref{GeneratedFs} we saw which assignments generated Fs.
But that will only catch the assignments that students did and failed, not 
assignments where students made no attempt.

We'll create a function that takes the module as an argument and then outputs 
the distribution of submissions for each assignment.
A submission is counted as a submission if it has a date.
<<functions>>=
submissions() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    <<let [[submissions]] be the up-to-date file with submissions>>
    <<calculate [[num_users]] from [[submissions]]>>
    echo
    echo "${course} ${module} submissions:"
    grep <<date regex>> "${submissions}" \
      | <<cut submission name and count>>
    <<print maximum possible>>
  done
}
<<let [[submissions]] be the up-to-date file with submissions>>=
local submissions=`fetch_submissions "$course" "$module"`
<<calculate [[num_users]] from [[submissions]]>>=
local num_users=`cat "$submissions" | cut -f 3 | sort -u | wc -l`
<<date regex>>=
-P '\d{4}-\d{2}-\d{2}'
<<cut submission name and count>>=
cut -f 2 | sort | uniq -c | sort -rn
<<print maximum possible>>=
echo "Out of maximum ${num_users}."
@

If we consider running [[submissions tilkry24 INL1]], the result looks like 
this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'submissions', 'tilkry24', 'INL1'])
\end{pycode}
But if we look at LAB1, which has optional assignments, we get this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'submissions', 'tilkry24', 'LAB1'])
\end{pycode}

We also add a function for the opposite, to count how many submissions an 
assignment is missing.
This is in fact more interesting to know.
<<functions>>=
missing() {
  local course="$1"
  shift
  local modules="$@"
  for module in $modules; do
    <<let [[submissions]] be the up-to-date file with submissions>>
    <<calculate [[num_users]] from [[submissions]]>>
    echo
    echo "${course} ${module} missing submissions:"
    grep -v <<date regex>> "${submissions}" \
      | <<cut submission name and count>>
    <<print maximum possible>>
  done
}
@

If we consider running [[missing tilkry24 LAB1]], the result looks like this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'missing', 'tilkry24', 'LAB1'])
\end{pycode}
We can see that the assignment \enquote{Secure multi-party computation} is the 
only (optional) assignment that no student did.

It makes the most sense to add this to the [[all]] function.
Because we want to see the grades and then the missing assignments, not the 
submissions.
<<call all functions>>=
echo
missing "$course" "$module"
@


\subsection{How many attempts did the students make to pass?}

We'll now add a function to count the number of attempts the students made on 
different assignments.
Since the passing grade might differ between assignments, we'll just count the 
total number of attempts for those who didn't receive a failing grade.
(It's easier to detect fails.)
We want the result to look like this:
\begin{pycode}
minted_output(['bash', 'results', 'attempts', 'tilkry25', 'INL1'])
\end{pycode}
<<functions>>=
attempts() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    local submissions=`fetch_history "$course" "$module"`
    echo
    <<for each [[assignment]] in [[submissions]]>>
      echo
      echo "${course} ${module} ${assignment} attempts:"
      <<print counts for attempts for [[assignment]]>>
    done
  done
}
<<call all functions>>=
echo
attempts "$course" "$module"
@

The output of [[fetch_history]] is similar to the output of 
[[fetch_submissions]].
So we can get the list of assignments in the same way: get that column and sort 
it uniquely.
Since the assignment names will contain spaces, we need to set the IFS to 
newline.
<<for each [[assignment]] in [[submissions]]>>=
local assignments=`cat "${submissions}" | cut -f 2 | sort -u`
oldIFS="$IFS"
IFS=$'\n'
for assignment in $assignments; do
  IFS="$oldIFS"
@

Lastly, we should print the counts.
We'll do this by first filtering out the submissions for the current 
assignment.
<<print counts for attempts for [[assignment]]>>=
cat "${submissions}" \
| cut -f 2- \
| grep "^${assignment}" \
<<filter out only those who passed>> \
@ Then we'll cut out the student names and count them.
The number of times a student's name occurs is the number of attempts they 
made.
<<print counts for attempts for [[assignment]]>>=
| cut -f 2 | sort | uniq -c \
@ Finally, we'll sort the attempts, count them and print them.
The printed counts will be sorted by the number of attempts.
<<print counts for attempts for [[assignment]]>>=
| sed -E "s/^ *([0-9]+).*$/\1/" | sort -n | uniq -c | sort -k 2 -n \
@ We'll also rewrite the line to make it more readable.
<<print counts for attempts for [[assignment]]>>=
| sed -E "s/([0-9]+)(\s+)([0-9]+)$/\1 student(s) submitted\2\3 time(s)/"
@

If we want to filter out only the students who passed, we can do that by 
filtering out only those who have a passing grade and then use their names to 
filter the history.
However, it's hard to tell what is a pass due to varying grading scales.
It's much easier to tell what a non-passing grade is.
So we remove everyone who has a grade which is not a pass.
<<filter out only those who passed>>=
| grep -f <(cat "${submissions}" | grep "${assignment}" \
            | cut -f 3-4 | grep -Ev "(Fx?|incomplete|\s)$" | cut -f 1)
@

\subsection{Testing on another course}

We'll now test the script on another course.
We'll use it on the LAB3 module of prgm23 and prgi23:
[[all prg[im]23 LAB3]].
\begin{pycode}
minted_output(['bash', 'results.sh', 'all', 'prg[im]23', 'LAB3'])
\end{pycode}


\subsection{Ungraded submissions}

We'll now add a function to list the number of ungraded submissions.
This will be similar to the [[missing]] function above, but will instead count 
the number of ungraded submissions.

A submission is ungraded when
\begin{enumerate}
\item it has a submission date, but no grade nor grading date; or
\item it has a grade and grading date, but it has a submission date newer than 
the grading date---but, if it has grade P or A, it's considered graded; 
otherwise it's ungraded.
\end{enumerate}

We want the result to look like this:
\begin{pycode}
minted_output(['bash', './results', 'ungraded', 'prgm24', 'LAB3'])
\end{pycode}

<<functions>>=
ungraded() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    <<let [[submissions]] be the up-to-date file with submissions>>
    <<calculate [[num_users]] from [[submissions]]>>
    echo
    echo "${course} ${module} ungraded submissions:"
    <<filter out ungraded submissions from [[submissions]]>> \
      | <<cut submission name and count>>
    <<print maximum possible>>
  done
}
@

We'll add a function to filter out the ungraded submissions.
It'll take the submissions as input on stdin and output only the ungraded lines 
(verbatim) on stdout.
<<filter out ungraded submissions from [[submissions]]>>=
cat "${submissions}" | filter_ungraded
@

To implement the function, we simply need to check the dates and grades 
according to the rules above.
The lines in the submissions file look like this:
\begin{minted}{text}
course  assignment  student  grade  submission-date  grading-date
\end{minted}
<<functions>>=
filter_ungraded() {
  awk -F '\t' '
    {
      if ($5 == "" && $6 == "") {
        print
      } else if ($5 != "" && $6 != "" && $5 > $6) {
        if ($4 != "P" && $4 != "A") {
          print
        }
      }
    }
  '
}
@

Since we have grades and fails when running the [[all]] function, it might be a 
good complement to also add the ungraded submissions.
<<call all functions>>=
echo
ungraded "$course" "$module"
@


\section{Transferring the results to LADOK}

A final feature that might be useful, although it's not about stats, is to 
report the results used by the stats to LADOK.
This function will work the same as others, take a course and a list of 
modules, then it will simply pipe the results to the [[ladok report]] command.
(It will also rewrite course names to course codes.)
<<functions>>=
to_ladok() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    local results=`fetch_results "$course" "$module"`
    cat "$results" \
      | <<remove Fs from the list>> \
      | <<rewrite course name to course code>> \
      | ladok report -fv
  done
}
@

To remove the Fs from the list we'll use the same pattern as in 
\cref{GeneratedFs}.
<<remove Fs from the list>>=
grep -v -E "\\s($FAILED_PATTERN)\\s"
@

For the tilkry24 course, the course name is \enquote{DD2520 VT24 (tilkry24)}.
So we'll rewrite this to \enquote{DD2520} before sending it to LADOK.
<<rewrite course name to course code>>=
sed -E "s/ ?[HV]T[0-9]{2}( \(.*\))?//"
@

\end{document}
