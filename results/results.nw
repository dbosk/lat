\documentclass[a4paper,article,oneside]{memoir}
\let\subsubsection\subsection
\let\subsection\section
\let\section\chapter

\input{preamble.tex}
\usepackage{didactic}

\title{Some stats for Canvas courses}
\author{Daniel Bosk}

\begin{document}
\maketitle
\tableofcontents
@

\begin{pycode}
import os
import subprocess

vars = {
  "tilkry24_LAB1": "canvaslms.grades.tilkryLAB1",
  "prgi23_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgm23_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgi24_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgm24_LAB2": "canvaslms.grades.maxgradesurvey"
}

def minted_output(command, shell=False):
  env = os.environ.copy()
  env.update(vars)

  output = subprocess.run(command, capture_output=True,
                          shell=shell, env=env)

  print(r'\begin{minted}{text}')
  stdout = output.stdout.decode('utf-8').strip()
  if stdout:
    print(stdout)
  stderr = output.stderr.decode('utf-8').strip()
  if stderr:
    print(stderr)
  print(r'\end{minted}')
\end{pycode}

\section{Some stats}

We'll give an overview of the stats here.
We'll use the tilkry24 course as an example.
In the following section we see how the results are computed.
In this section we'll just run the script and show the output here.

\subsection{Results on INL1}

The INL1 module has only mandatory assignments.
Let's start with the overall grades:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'INL1'])
\end{pycode}

We can see that there are some Fs.
Let's see which assignments generated Fs:
\begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'INL1'])
\end{pycode}
Most are due to the quiz.

\subsection{Results on LAB1}

Let's move on to LAB1.
This is the more complex part where there are optional assignments, and 
depending on how the student does they'll get a grade in the scale between A 
and F.
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'LAB1'])
\end{pycode}
Most students did well.
Particularly many got an A or B.

Let's see which assignments generated Fs:
\begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'LAB1'])
\end{pycode}
Only one.
This is probably due to the students not having submitted some of the mandatory 
assignments.
Therefore, let's have a look at which assignments missed the most submissions:
\begin{pycode}
minted_output(['bash', 'results.sh', 'missing', 'tilkry24', 'LAB1'])
\end{pycode}
We see that rather many of the mandatory assignments are missing submissions.


\section{The script, or, how the results are made}

We'll write a short shell script that fetches the results from Canvas and 
summarizes them in statistics.
This will be done in the same way as they are reported to LADOK, so they'll be 
representative.
<<results.sh>>=
#!/bin/bash

<<constants>>
<<functions>>

main() {
  <<call specified function>>
}

# call main if not sourced
# https://stackoverflow.com/a/28776166/1305099
(return 0 2>/dev/null) || main "$@"
@

We'll also make calls to that code in this documentation, to get the results 
and how they were generated.
<<Makefile>>=
<<Makefile variables>>

.PHONY: all
all: results.pdf results

results.pdf: results.tex results.sh
results: results.sh
	ln -f $< $@
	chmod +x $@
@

We'll also build this Makefile.
We're fortunately enough that [[make]] tries to remake any Makefile that it 
loads.
<<Makefile>>=
Makefile: results.nw
	${NOTANGLE.mk}
@

We'll execute the script in the documentation using PythonTeX, so we need the 
[[-shell-escape]] flag.
<<Makefile variables>>=
LATEXFLAGS+=  -shell-escape
@

We'll also use the [[didactic]] package.
We'll use the one from the repo instead of an installed version.
<<Makefile>>=
results.pdf: didactic.sty

INCLUDE_DIDACTIC=../didactic
include ${INCLUDE_DIDACTIC}/didactic.mk
@

Finally, we'll use some automation to build.
<<Makefile>>=
INCLUDE_MAKEFILES=../makefiles
include ${INCLUDE_MAKEFILES}/tex.mk
include ${INCLUDE_MAKEFILES}/noweb.mk
@


\subsection{Calling the functions}

We'll take the function name as the first argument and simply call it.
If we don't have any arguments, we'll print a help text.
<<call specified function>>=
[[ "$1" = "" ]] \
  && <<echo help text>> \
  || "$@"
<<echo help text>>=
echo -e "Usage: $0 [function]\nFunctions:\n<<list all functions>>"
<<list all functions>>=
$(sed -En 's/^(\w+)\(\) \{/  \1/p' $0 | grep -Pv '^\s*main$')
@

We'll also add a function that runs all the most interesting functions.
<<functions>>=
all() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    echo
    <<call all functions>>
  done
}
@


\section{Implementation of the stats}

Now we'll go through the implementation of the stats functions.

\subsection{Fetching the results}

We'll now defined two functions that can be used to fetch the desired results.
These will also act as a cache, so that we don't fetch results more than once.

The functions will take two arguments, the course and the module.
On tilkry24 we have two modules: LAB1 and INL1.

LAB1 needs a particular summary module to compute the final grade ([[-S]] 
option to [[canvaslms results]]).
So we need to treat that with a special case.
But INL1 can use the default.

We'll use a function to fetch the results for each module.
It takes the course and module as an argument.

We'll compute the grades using [[canvaslms results]], which is the same way as 
the results are reported to LADOK.
However, this takes time, so we'll only do it if the file doesn't already 
exist.
This way each function can call the same function to fetch the results, but it 
will slow down only if they don't already exist.
<<functions>>=
fetch_results() {
  local course="$1"
  local module="$2"
  local file="results-${course}-${module}.csv"

  echo "$file"
  [[ -f "$file" ]] && return

  <<variables for summary module>>

  canvaslms results -c "${course}" -F '' \
    -A "^${module}" \
    <<add [[-S]] option if needed>> \
    > "${file}"
}
@

If the module needs a summary module, we'll handle it by setting a particular 
environment variable.
As mentioned above, tilkry24 LAB1 needed a special summary module.
If we set
\begin{center}
  [[tilkry24_LAB1=canvaslms.grades.tilkryLAB1]]
\end{center}
the function will use it.
<<variables for summary module>>=
local summary_module=`get_summary_module "${course}" "${module}" 2> /dev/null`
<<functions>>=
get_summary_module() {
  local course="$1"
  local module="$2"
  local var_name="${course}_${module}"
  echo "${!var_name}"
}
<<add [[-S]] option if needed>>=
$([[ -n "$summary_module" ]] && echo "-S $summary_module")
@

We'll do the same for fetching submission results for the individual 
assignments (not the module as a whole).
Unlike the previous one, there is no special case here.
<<functions>>=
fetch_submissions() {
  local course="$1"
  local module="$2"
  local file="submissions-${course}-${module}.csv"

  echo "$file"
  [[ -f "$file" ]] && return

  canvaslms submissions -c "${course}" \
    -A "^${module}" \
    > "${file}"
}
@

Finally, we also want to be able to clear the results cache.
We call this function to refetch the results, to sound less dramatic\footnote{%
  Clean and clear might sound like we're deleting the results, but we're just 
  refetching them.
}.
<<functions>>=
refetch() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    rm -f "results-${course}-${module}.csv"
    rm -f "submissions-${course}-${module}.csv"
  done
}
@

Now let's look at the stats.
We'll add one function per stat we want.

\subsection{Grades of modules}

We'll start by looking at the grades of the modules.
We'll create a function that generates stats for a module, where the module is 
passed as an argument.
<<functions>>=
grades() {
  local course="$1"
  shift
  local modules="$@"
  
  for module in $modules; do
    local results=`fetch_results "$course" "$module"`
    echo
    echo "${course} ${module} grades:"
    cat "$results" | cut -f 4 | sort | uniq -c
  done
}
@

If we consider running [[grades tilkry24 LAB1 INL1]], the result looks like 
this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'LAB1', 'INL1'])
\end{pycode}


\subsection{Which assignments generated Fs?}\label{GeneratedFs}

We want a function that takes the course and a module as an argument and then 
outputs the distribution of Fs for the assignments.
We'll do this by filtering out the Fs and then count the occurrences of each 
assignment.
<<functions>>=
fails() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    local submissions=`fetch_submissions "$course" "$module"`
    echo
    echo "${course} ${module} Fs:"
    grep <<pattern for failed grades in [[submissions]]>> \
      | cut -f 2 | sort | uniq -c \
      | sort -n
  done
}
<<pattern for failed grades in [[submissions]]>>=
-E "\\s($FAILED_PATTERN)\\s" "$submissions"
<<constants>>=
FAILED_PATTERN='Fx?|incomplete'
@

If we consider running [[fails tilkry24 INL1]], the result looks like this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'INL1'])
\end{pycode}

\subsection{Adding grades and fails to \texttt{all}}

We'll start by looking at the grades of LAB1, which is also the final grade.
<<call all functions>>=
echo
grades "$course" "$module"
echo
fails "$course" "$module"
@

\subsection{Which assignments were the least popular?}

We now want to see which assignments the students did and which they did not.
In \cref{GeneratedFs} we saw which assignments generated Fs.
But that will only catch the assignments that students did and failed, not 
assignments where students made no attempt.

We'll create a function that takes the module as an argument and then outputs 
the distribution of submissions for each assignment.
A submission is counted as a submission if it has a date.
<<functions>>=
submissions() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    <<update submissions file [[submissions]]>>
    <<calculate [[num_users]] from [[submissions]]>>
    echo
    echo "${course} ${module} submissions:"
    grep <<date in [[submissions]]>> \
      | <<cut submission name and count>>
    <<print maximum possible>>
  done
}
<<update submissions file [[submissions]]>>=
local submissions=`fetch_submissions "$course" "$module"`
<<calculate [[num_users]] from [[submissions]]>>=
local num_users=`cat "$submissions" | cut -f 3 | sort -u | wc -l`
<<date in [[submissions]]>>=
-P '\d{4}-\d{2}-\d{2}' "$submissions"
<<cut submission name and count>>=
cut -f 2 | sort | uniq -c | sort -rn
<<print maximum possible>>=
echo "Out of maximum ${num_users}."
@

If we consider running [[submissions tilkry24 INL1]], the result looks like 
this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'submissions', 'tilkry24', 'INL1'])
\end{pycode}
But if we look at LAB1, which has optional assignments, we get this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'submissions', 'tilkry24', 'LAB1'])
\end{pycode}

We also add a function for the opposite, to count how many submissions an 
assignment is missing.
This is in fact more interesting to know.
<<functions>>=
missing() {
  local course="$1"
  shift
  local modules="$@"
  for module in $modules; do
    <<update submissions file [[submissions]]>>
    <<calculate [[num_users]] from [[submissions]]>>
    echo
    echo "${course} ${module} missing submissions:"
    grep -v -P <<date in [[submissions]]>> \
      | <<cut submission name and count>>
    <<print maximum possible>>
  done
}
@

If we consider running [[missing tilkry24 LAB1]], the result looks like this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'missing', 'tilkry24', 'LAB1'])
\end{pycode}
We can see that the assignment \enquote{Secure multi-party computation} is the 
only (optional) assignment that no student did.

It makes the most sense to add this to the [[all]] function.
Because we want to see the grades and then the missing assignments, not the 
submissions.
<<call all functions>>=
echo
missing "$course" "$module"
@

\subsection{Testing on another course}

We'll now test the script on another course.
We'll use it on the LAB3 module of prgm23 and prgi23:
[[all prg[im]23 LAB3]].
\begin{pycode}
minted_output(['bash', 'results.sh', 'all', 'prg[im]23', 'LAB3'])
\end{pycode}


\section{Transferring the results to LADOK}

A final feature that might be useful, although it's not about stats, is to 
report the results used by the stats to LADOK.
This function will work the same as others, take a course and a list of 
modules, then it will simply pipe the results to the [[ladok report]] command.
(It will also rewrite course names to course codes.)
<<functions>>=
to_ladok() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    local results=`fetch_results "$course" "$module"`
    cat "$results" \
      | <<remove Fs from the list>> \
      | <<rewrite course name to course code>> \
      | ladok report -fv
  done
}
@

To remove the Fs from the list we'll use the same pattern as in 
\cref{GeneratedFs}.
<<remove Fs from the list>>=
grep -v -E "\\s($FAILED_PATTERN)\\s"
@

For the tilkry24 course, the course name is \enquote{DD2520 VT24 (tilkry24)}.
So we'll rewrite this to \enquote{DD2520} before sending it to LADOK.
<<rewrite course name to course code>>=
sed -E "s/ ?[HV]T[0-9]{2}( \(.*\))?//"
@

\end{document}
