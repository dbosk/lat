\documentclass[a4paper,article,oneside]{memoir}
\let\subsubsection\subsection
\let\subsection\section
\let\section\chapter

\input{preamble.tex}
\usepackage{didactic}

\title{Some stats for courses}
\author{Daniel Bosk}

\begin{document}
\maketitle
\tableofcontents*
\clearpage
@

\begin{pycode}
import os
import subprocess

vars = {
  "RESULTS_CACHE": ".",
  "tilkry24_LAB1": "canvaslms.grades.tilkryLAB1",
  "tilkry25_LAB1": "canvaslms.grades.tilkryLAB1",
  "prgi22_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgm22_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgi23_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgm23_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgi24_LAB2": "canvaslms.grades.maxgradesurvey",
  "prgm24_LAB2": "canvaslms.grades.maxgradesurvey"
}

def minted_output(command, shell=False):
  env = os.environ.copy()
  env.update(vars)

  output = subprocess.run(command, capture_output=True,
                          shell=shell, env=env)

  print(r'\begin{minted}{text}')
  stdout = output.stdout.decode('utf-8').strip()
  if stdout:
    print(stdout)
  stderr = output.stderr.decode('utf-8').strip()
  if stderr:
    print(stderr)
  print(r'\end{minted}')
\end{pycode}

\section{Some stats from the tilkry course}

We'll give an overview of the stats here.
We'll use the tilkry24 and tilkry25 courses\footnote{%
  The DD2520 Applied Cryptography course (tilkry) is a master's level course.
  It usually has 50--80 participants per year.
} as examples.
In the following section we see how the results are computed.
In this section we'll just run the script and show the output here.

\subsection{Results on INL1}

The INL1 module has only mandatory assignments.
Let's start with the overall grades.
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades tilkry24 INL1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'INL1'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades tilkry25 INL1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry25', 'INL1'])
  \end{pycode}
\end{minipage}
\\[.5em]
\end{flushleft}

We can see that there are some Fs.
Let's see which assignments generated Fs.
\begin{fullwidth}
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results fails tilkry24 INL1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'INL1'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results fails tilkry25 INL1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry25', 'INL1'])
  \end{pycode}
\end{minipage}
\\[.5em]
\end{flushleft}
\end{fullwidth}
Most are due to the quiz.

On tilkry24 they only had one chance on the INL1Quiz, but on tilkry25 they got 
to retry until they passed.
We want to see how many attempts the students made to pass the quiz.
[[results attempts tilkry25 INL1]] yields:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'tilkry25', 'INL1'])
\end{pycode}
We can compare that to the results on tilkry24,
[[results attempts tilkry24 INL1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'tilkry24', 'INL1'])
\end{pycode}

\subsection{Results on LAB1}

Let's move on to LAB1.
This is the more complex part where there are optional assignments, and 
depending on how the student does they'll get a grade in the scale between A 
and F.
We'll start with the overall grades.
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades tilkry24 LAB1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'LAB1'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades tilkry25 LAB1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry25', 'LAB1'])
  \end{pycode}
\end{minipage}
\\[.5em]
\end{flushleft}
Most students did well.
Particularly many got an A or B.

Let's see which assignments generated Fs,
[[results fails tilkry24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'LAB1'])
\end{pycode}
Only one.
This is probably due to the students not having submitted some of the mandatory 
assignments.
Therefore, let's have a look at which assignments missed the most submissions,
[[results missing tilkry24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'missing', 'tilkry24', 'LAB1'])
\end{pycode}
We see that rather many of the mandatory assignments are missing submissions.

To tilkry25 we made the presentation of AES mandatory.
This forced the students to pick one more lab for a higher grade.
This changed the distribution of missing submissions.
[[results missing tilkry25 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'missing', 'tilkry25', 'LAB1'])
\end{pycode}

\subsection{Throughput}

We also want to see how many students passed the course by the end of the 
course and how many passed the course after the course ended.
We can use the [[results throughput tilkry24 -r 61703]] command to see that.
\begin{flushleft}
  [[results throughput DD2520 -r 61703]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'throughput', 'DD2520', '-r', '61703'])
  \end{pycode}
\end{flushleft}

\section{Some stats from the prgm and prgi courses}

We'll now test the script on other courses.
We'll use the introductory programming courses---prgm23, prgi23, prgm24 and 
prgi24---as examples\footnote{%
  The DD1310 (prgm, for maskinteknik) and DD1317 (prgi, for industriell 
  ekonomi) courses are introductory programming courses---the first the 
  students take on their programmes---with 220 and 180 participants, 
  respectively.
}.

\subsection{Results on LAB1}

We want to compare the results on LAB1 for prgm23, prgm24, prgi23 and prgi24.
So we can run [[all]] on each one separately and then compare the results.
However, it's nicer to do each individually so that we can compare more easily.

Let's start with the grades.
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades prgm23 LAB1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm23', 'LAB1'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades prgi23 LAB1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi23', 'LAB1'])
  \end{pycode}
\end{minipage}
\\[1.5em]
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades prgm24 LAB1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm24', 'LAB1'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades prgi24 LAB1]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi24', 'LAB1'])
  \end{pycode}
\end{minipage}
\\[.5em]
\end{flushleft}

Next we want to see the number of attempts.
\begin{fullwidth}
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
[[results attempts prgm23 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm23', 'LAB1'])
\end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
[[results attempts prgi23 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi23', 'LAB1'])
\end{pycode}
\end{minipage}

\begin{minipage}[t]{0.45\columnwidth}
[[results attempts prgm24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm24', 'LAB1'])
\end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
[[results attempts prgi24 LAB1]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi24', 'LAB1'])
\end{pycode}
\end{minipage}
\end{flushleft}
\end{fullwidth}


\subsection{Results on LAB2}

We also want to compare the results on LAB2 for prgm23, prgm24, prgi23 and 
prgi24.
But here we want to see the number of attempts of those who passed\footnote{%
  Due to the construction of the Datorprov setup, we must run the attempts on 
  another assignment group than LAB2.
  Because LAB2 only contains the final grade, which doesn't account for the 
  number of attempts.
} and how many failed.
Let's start with the grade distributions.
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
[[results grades prgm23 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm23', 'LAB2'])
\end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
[[results grades prgi23 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi23', 'LAB2'])
\end{pycode}
\end{minipage}
\\[1.5em]
\begin{minipage}[t]{0.45\columnwidth}
[[results grades prgm24 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm24', 'LAB2'])
\end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
[[results grades prgi24 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi24', 'LAB2'])
\end{pycode}
\end{minipage}
\\[.5em]
\end{flushleft}

Now, let's have a look at the number of attempts.
Here we can't use LAB2, because the submissions are not done in any assignment 
there.
The assignments are distributed as follows.
\begin{verbatim}
DD1317 HT23 (prgi23)	LAB2	Datorprov
DD1317 HT24 (prgi24)	LAB2	Betyg datorprovet
DD1317 HT24 (prgi24)	Datorprov	Exempelprov
DD1317 HT24 (prgi24)	Datorprov	Datorprov (v2024)
DD1317 HT24 (prgi24)	Datorprov	Inledande diagnostiskt prov
DD1310 HT23 (prgm23)	LAB2	Betyg datorprovet
DD1310 HT23 (prgm23)	Imported Assignments	Datorprov (oktober)
DD1310 HT23 (prgm23)	Imported Assignments	Datorprov (december och augusti)
DD1310 HT24 (prgm24)	Bonus	Ackumulerad datorprovspoäng
DD1310 HT24 (prgm24)	LAB2	Betyg datorprovet
DD1310 HT24 (prgm24)	Datorprov	Datorprov
DD1310 HT24 (prgm24)	Datorprov	Exempelprov
DD1310 HT24 (prgm24)	Datorprov	Inledande diagnostiskt prov
\end{verbatim}
We see that for prgm23, prgm24, prgi24 the \enquote{Datorprov} assignment is 
not in LAB2, only \enquote{Betyg datorprovet} is.
But the attempts are in the \enquote{Datorprov} assignment, which is in the 
\enquote{Datorprov} assignment group.
(A script transfers the results from the \enquote{Datorprov} assignment to the 
\enquote{Betyg datorprovet} assignment, but Canvas only counts that as changing 
the grade of \emph{one} submission.)
However, for prgi23, the \enquote{Datorprov} assignment is in LAB2.
It was the same for prgm22.

Let's turn to the results.
Let's see how the number of attempts have changed over the years.
\begin{fullwidth}
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
[[results attempts prgm22 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm22', 'LAB2'])
\end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
[[results attempts prgi22 LAB2]]:
\begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi22', 'LAB2'])
\end{pycode}
\end{minipage}
\\[.5em]
\end{flushleft}
\end{fullwidth}
So far, prgm22 and prgi22 did the same test.
However, for prgm23 we changed the test.
Now they only had two chances, but could earn bonus points.
Those who got an Fx could do only that question on the next test.
Others had to retake the whole test.

For prgm24, everyone only needed to retake the questions that they failed.
Otherwise it was the same as for prgm24.
They also had more than two chances.
\begin{fullwidth}
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results attempts prgm23 Datorprov]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm23', 'Datorprov'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results attempts prgm24 Datorprov]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm24', 'Datorprov'])
  \end{pycode}
\end{minipage}
\\[.5em]
\end{flushleft}
\end{fullwidth}

But we kept the old test for prgi23 and switched to the same test in prgi24 as 
prgm23 and prgm24.
For both prgi23 and prgi24, the students could take the test once a month 
throughout the year.
They didn't have the exam proctored.
They also couldn't earn bonus points for the test.
\begin{fullwidth}
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results attempts prgi23 LAB2]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi23', 'LAB2'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results attempts prgi24 Datorprov]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi24', 'Datorprov'])
  \end{pycode}
\end{minipage}
\\[.5em]
\end{flushleft}
\end{fullwidth}
For prgi25, the test will be proctored, same as for prgm23, prgm24, prgm25.

\subsection{Results on LAB3}

We also want to compare the results on LAB3 for prgm23, prgm24, prgi23 and 
prgi24.
Let's start with the grades.
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades prgm23 LAB3]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm23', 'LAB3'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades prgi23 LAB3]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi23', 'LAB3'])
  \end{pycode}
\end{minipage}
\end{flushleft}
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades prgm24 LAB3]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgm24', 'LAB3'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results grades prgi24 LAB3]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'prgi24', 'LAB3'])
  \end{pycode}
\end{minipage}
\end{flushleft}

Next we want to see the number of attempts.
\begin{fullwidth}
\begin{flushleft}
\begin{minipage}[t]{0.45\columnwidth}
  [[results attempts prgm23 LAB3]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm23', 'LAB3'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results attempts prgi23 LAB3]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi23', 'LAB3'])
  \end{pycode}
\end{minipage}
\\[1.5em]
\begin{minipage}[t]{0.45\columnwidth}
  [[results attempts prgm24 LAB3]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgm24', 'LAB3'])
  \end{pycode}
\end{minipage}
\begin{minipage}[t]{0.45\columnwidth}
  [[results attempts prgi24 LAB3]]:
  \begin{pycode}
minted_output(['bash', 'results.sh', 'attempts', 'prgi24', 'LAB3'])
  \end{pycode}
\end{minipage}
\end{flushleft}
\end{fullwidth}



\section{The script, or, how the results are made}

We'll write a short shell script that fetches the results from Canvas and 
summarizes them in statistics.
This will be done in the same way as they are reported to LADOK, so they'll be 
representative.
<<results.sh>>=
#!/bin/bash

<<constants>>
<<functions>>

main() {
  <<call specified function>>
}

# call main if not sourced
# https://stackoverflow.com/a/28776166/1305099
(return 0 2>/dev/null) || main "$@"
@

We'll also make calls to that code in this documentation, to get the results 
and how they were generated.
<<Makefile>>=
<<Makefile variables>>

.PHONY: all
all: results.pdf results

results.pdf: results.tex results.sh
results: results.sh
	ln -f $< $@
	chmod +x $@
@

We'll also build this Makefile.
We're fortunately enough that [[make]] tries to remake any Makefile that it 
loads.
<<Makefile>>=
Makefile: results.nw
	${NOTANGLE.mk}
@

We'll execute the script in the documentation using PythonTeX, so we need the 
[[-shell-escape]] flag.
<<Makefile variables>>=
LATEXFLAGS+=  -shell-escape
@

We'll also use the [[didactic]] package.
We'll use the one from the repo instead of an installed version.
<<Makefile>>=
results.pdf: didactic.sty

INCLUDE_DIDACTIC=../didactic
include ${INCLUDE_DIDACTIC}/didactic.mk
@

Finally, we'll use some automation to build.
<<Makefile>>=
INCLUDE_MAKEFILES=../makefiles
include ${INCLUDE_MAKEFILES}/tex.mk
include ${INCLUDE_MAKEFILES}/noweb.mk
@


\subsection{Calling the functions}

We'll take the function name as the first argument and simply call it.
If we don't have any arguments, we'll print a help text.
<<call specified function>>=
[[ "$1" = "" ]] \
  && <<echo help text>> \
  || "$@"
<<echo help text>>=
echo -e "Usage: $0 [function]\nFunctions:\n<<list all functions>>"
<<list all functions>>=
$(sed -En 's/^(\w+)\(\) \{/  \1/p' $0 | grep -Pv '^\s*main$')
@

We'll also add a function that runs all the most interesting functions.
<<functions>>=
all() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    echo
    <<call all functions>>
  done
}
@


\section{Implementation of the stats}

Now we'll go through the implementation of the stats functions.

\subsection{Fetching the results}

We'll now defined two functions that can be used to fetch the desired results.
These will also act as a cache, so that we don't fetch results more than once.

The functions will take two arguments, the course and the module.
On tilkry24 we have two modules: LAB1 and INL1.

LAB1 needs a particular summary module to compute the final grade ([[-S]] 
option to [[canvaslms results]]).
So we need to treat that with a special case.
But INL1 can use the default.

We'll use a function to fetch the results for each module.
It takes the course and module as an argument.

We'll compute the grades using [[canvaslms results]], which is the same way as 
the results are reported to LADOK.
However, this takes time, so we'll only do it if the file doesn't already 
exist.
This way each function can call the same function to fetch the results, but it 
will slow down only if they don't already exist.
<<functions>>=
fetch_results() {
  local course="$1"
  local module="$2"
  local file="${cache_dir}/results-${course}-${module}.csv"

  echo "$file"
  [[ -s "$file" ]] && return

  <<variables for summary module>>

  canvaslms results \
    -c "${course}" -F '' \
    -A "^${module}" \
    <<add [[-S]] option if needed>> \
    > "${file}"
}
@

We want to cache the results, so we'll set a cache directory.
This way we can control where the cache is stored.
For instance, if we set it to [[/tmp]], the cache will be removed when the 
system is rebooted or a sufficiently long time has passed.
But we might just as well set it to [[.]] to keep the cache in the current 
working directory.

We will also let the user set the cache directory by setting an environment 
variable.
If that is set we'll use it, otherwise we'll use [[/tmp]] by default.
<<constants>>=
cache_dir="${RESULTS_CACHE:-/tmp}"
@

If the module needs a summary module, we'll handle it by setting a particular 
environment variable.
As mentioned above, tilkry24 LAB1 needed a special summary module.
If we set
\begin{center}
  [[tilkry24_LAB1=canvaslms.grades.tilkryLAB1]]
\end{center}
the function will use it.
<<variables for summary module>>=
local summary_module=`get_summary_module "${course}" "${module}" 2> /dev/null`
<<functions>>=
get_summary_module() {
  local course="$1"
  local module="$2"
  local var_name="${course}_${module}"
  echo "${!var_name}"
}
<<add [[-S]] option if needed>>=
$([[ -n "$summary_module" ]] && echo "-S $summary_module")
@

We'll do the same for fetching submission results for the individual 
assignments (not the module as a whole).
Unlike the previous one, there is no special case here.
<<functions>>=
fetch_submissions() {
  local course="$1"
  local module="$2"
  local file="${cache_dir}/submissions-${course}-${module}.csv"

  echo "$file"
  [[ -s "$file" ]] && return

  canvaslms submissions \
    -c "${course}" \
    -A "^${module}" \
    > "${file}"
}
@

We want to construct [[fetch_history]] similarly to [[fetch_submissions]], with 
a cache.
The only difference to the others is that we use the [[-H]] option to get the 
history of submissions instead of only the latest.
<<functions>>=
fetch_history() {
  local course="$1"
  local module="$2"
  local file="${cache_dir}/history-${course}-${module}.csv"

  echo "$file"
  [[ -s "$file" ]] && return

  canvaslms submissions \
    -c "${course}" \
    -A "^${module}" \
    -H \
    > "${file}"
}
@

Finally, we also want to be able to clear the results cache.
We call this function to refetch the results, to sound less dramatic\footnote{%
  Clean and clear might sound like we're deleting the results, but we're just 
  refetching them.
}.
<<functions>>=
refetch() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    rm -f "${cache_dir}/results-${course}-${module}.csv"
    rm -f "${cache_dir}/submissions-${course}-${module}.csv"
    rm -f "${cache_dir}/history-${course}-${module}.csv"
  done
}
@

Now let's look at the stats.
We'll add one function per stat we want.

\subsection{Grades of modules}

We'll start by looking at the grades of the modules.
We'll create a function that generates stats for a module, where the module is 
passed as an argument.
<<functions>>=
grades() {
  local course="$1"
  shift
  local modules="$@"
  
  for module in $modules; do
    local results=`fetch_results "$course" "$module"`
    echo
    echo "${course} ${module} grades:"
    cat "$results" | cut -f 4 | sort | uniq -c
  done
}
@

If we consider running [[grades tilkry24 LAB1 INL1]], the result looks like 
this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'grades', 'tilkry24', 'LAB1', 'INL1'])
\end{pycode}

This is useful enough to add to the [[all]] function.
<<call all functions>>=
echo
grades "$course" "$module"
@

\subsection{Which assignments generated Fs?}\label{GeneratedFs}

We want a function that takes the course and a module as an argument and then 
outputs the distribution of Fs for the assignments.
We'll do this by filtering out the Fs and then count the occurrences of each 
assignment.
<<functions>>=
fails() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    local submissions=`fetch_submissions "$course" "$module"`
    echo
    echo "${course} ${module} Fs:"
    grep <<pattern for failed grades>> "$submissions" \
      | cut -f 2 | sort | uniq -c \
      | sort -n
  done
}
<<pattern for failed grades>>=
-E "\\s($FAILED_PATTERN)\\s"
<<constants>>=
FAILED_PATTERN='Fx?|incomplete'
@

If we consider running [[fails tilkry24 INL1]], the result looks like this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'fails', 'tilkry24', 'INL1'])
\end{pycode}

This is also useful enough to add to the [[all]] function.
<<call all functions>>=
echo
fails "$course" "$module"
@

\subsection{Which assignments were the least popular?}

We now want to see which assignments the students did and which they did not.
In \cref{GeneratedFs} we saw which assignments generated Fs.
But that will only catch the assignments that students did and failed, not 
assignments where students made no attempt.

We'll create a function that takes the module as an argument and then outputs 
the distribution of submissions for each assignment.
A submission is counted as a submission if it has a date.
<<functions>>=
submissions() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    <<let [[submissions]] be the up-to-date file with submissions>>
    <<calculate [[num_users]] from [[submissions]]>>
    echo
    echo "${course} ${module} submissions:"
    grep <<date regex>> "${submissions}" \
      | <<cut submission name and count>>
    <<print maximum possible>>
  done
}
<<let [[submissions]] be the up-to-date file with submissions>>=
local submissions=`fetch_submissions "$course" "$module"`
<<calculate [[num_users]] from [[submissions]]>>=
local num_users=`cat "$submissions" | cut -f 3 | sort -u | wc -l`
<<date regex>>=
-P '\d{4}-\d{2}-\d{2}'
<<cut submission name and count>>=
cut -f 2 | sort | uniq -c | sort -rn
<<print maximum possible>>=
echo "Out of maximum ${num_users}."
@

If we consider running [[submissions tilkry24 INL1]], the result looks like 
this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'submissions', 'tilkry24', 'INL1'])
\end{pycode}
But if we look at LAB1, which has optional assignments, we get this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'submissions', 'tilkry24', 'LAB1'])
\end{pycode}

We also add a function for the opposite, to count how many submissions an 
assignment is missing.
This is in fact more interesting to know.
<<functions>>=
missing() {
  local course="$1"
  shift
  local modules="$@"
  for module in $modules; do
    <<let [[submissions]] be the up-to-date file with submissions>>
    <<calculate [[num_users]] from [[submissions]]>>
    echo
    echo "${course} ${module} missing submissions:"
    grep -v <<date regex>> "${submissions}" \
      | <<cut submission name and count>>
    <<print maximum possible>>
  done
}
@

If we consider running [[missing tilkry24 LAB1]], the result looks like this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'missing', 'tilkry24', 'LAB1'])
\end{pycode}
We can see that the assignment \enquote{Secure multi-party computation} is the 
only (optional) assignment that no student did.

It makes the most sense to add this to the [[all]] function.
Because we want to see the grades and then the missing assignments, not the 
submissions.
<<call all functions>>=
echo
missing "$course" "$module"
@


\subsection{How many attempts did the students make to pass?}

We'll now add a function to count the number of attempts the students made on 
different assignments.
Since the passing grade might differ between assignments, we'll just count the 
total number of attempts for those who didn't receive a failing grade.
(It's easier to detect fails.)
We want the result to look like this:
\begin{pycode}
minted_output(['bash', 'results', 'attempts', 'tilkry25', 'INL1'])
\end{pycode}
<<functions>>=
attempts() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    local submissions=`fetch_history "$course" "$module"`
    echo
    <<for each [[assignment]] in [[submissions]]>>
      echo
      echo "${course} ${module} ${assignment}:"
      <<print counts for attempts for [[assignment]]>>
    done
  done
}
<<call all functions>>=
echo
attempts "$course" "$module"
@

The output of [[fetch_history]] is similar to the output of 
[[fetch_submissions]].
So we can get the list of assignments in the same way: get that column and sort 
it uniquely.
Since the assignment names will contain spaces, we need to set the IFS to 
newline.
<<for each [[assignment]] in [[submissions]]>>=
local assignments=`cat "${submissions}" | cut -f 2 | sort -u`
oldIFS="$IFS"
IFS=$'\n'
for assignment in $assignments; do
  IFS="$oldIFS"
@

Lastly, we should print the counts.
We'll do this by first filtering out the relevant submissions for the current 
assignment.
<<print counts for attempts for [[assignment]]>>=
cat "${submissions}" \
| cut -f 2- \
| grep "^${assignment}" \
| <<remove non-graded submissions>> \
| <<filter out only those who passed>> \
@ Then we'll cut out the student names and count them.
The number of times a student's name occurs is the number of attempts they 
made.
<<print counts for attempts for [[assignment]]>>=
| cut -f 2 | sort | uniq -c \
@ Finally, we'll sort the attempts, count them and print them.
The printed counts will be sorted by the number of attempts.
<<print counts for attempts for [[assignment]]>>=
| sed -E "s/^ *([0-9]+).*$/\1/" | sort -n | uniq -c | sort -k 2 -n \
@ We'll also rewrite the line to make it more readable.
<<print counts for attempts for [[assignment]]>>=
| sed -E "s/([0-9]+)(\s+)([0-9]+)$/\1 student(s) needed \2\3 attempt(s)/"
@

Now, we only want to include graded submissions.
Maybe the student submits one version, updates and submits another version five 
minutes later.
Then one of those versions will not be graded, so it's not really an attempt.
So we remove those submissions which are not graded, that is those that lack a 
grading date (the last column).
This means that we grep for submissions with two dates.
<<remove non-graded submissions>>=
grep -E '\s[0-9]{4}-[0-9]{2}-[0-9]{2}.*\s[0-9]{4}-[0-9]{2}-[0-9]{2}.*$'
@

If we want to filter out only the students who passed, we can do that by 
filtering out only those who have a passing grade and then use their names to 
filter the history.
However, it's hard to tell what is a pass due to varying grading scales.
It's much easier to tell what a non-passing grade is.
So we remove everyone who has a grade which is not a pass.
<<filter out only those who passed>>=
grep -f <(cat "${submissions}" | grep "${assignment}" \
          | cut -f 3-4 | grep -Ev "(Fx?|incomplete|\s)$" | cut -f 1)
@

\subsection{Ungraded submissions}

We'll now add a function to list the number of ungraded submissions.
This will be similar to the [[missing]] function above, but will instead count 
the number of ungraded submissions.

A submission is ungraded when
\begin{enumerate}
\item it has a submission date, but no grade nor grading date; or
\item it has a grade and grading date, but it has a submission date newer than 
the grading date---but, if it has grade P or A, it's considered graded; 
otherwise it's ungraded.
\end{enumerate}

We want the result to look like this:
\begin{pycode}
minted_output(['bash', './results', 'ungraded', 'prgm24', 'LAB3'])
\end{pycode}

<<functions>>=
ungraded() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    <<let [[submissions]] be the up-to-date file with submissions>>
    <<calculate [[num_users]] from [[submissions]]>>
    echo
    echo "${course} ${module} ungraded submissions:"
    <<filter out ungraded submissions from [[submissions]]>> \
      | <<cut submission name and count>>
    <<print maximum possible>>
  done
}
@

We'll add a function to filter out the ungraded submissions.
It'll take the submissions as input on stdin and output only the ungraded lines 
(verbatim) on stdout.
<<filter out ungraded submissions from [[submissions]]>>=
cat "${submissions}" | filter_ungraded
@

To implement the function, we simply need to check the dates and grades 
according to the rules above.
The lines in the submissions file look like this:
\begin{minted}{text}
course  assignment  student  grade  submission-date  grading-date
\end{minted}
<<functions>>=
filter_ungraded() {
  awk -F '\t' '
    {
      if ($5 == "" && $6 == "") {
        print
      } else if ($5 != "" && $6 != "" && $5 > $6) {
        if ($4 != "P" && $4 != "A") {
          print
        }
      }
    }
  '
}
@

Since we have grades and fails when running the [[all]] function, it might be a 
good complement to also add the ungraded submissions.
<<call all functions>>=
echo
ungraded "$course" "$module"
@


\section{Transferring the results to LADOK}

A final feature that might be useful, although it's not about stats, is to 
report the results used by the stats to LADOK.
This function will work the same as others, take a course and a list of 
modules, then it will simply pipe the results to the [[ladok report]] command.
(It will also rewrite course names to course codes.)
<<functions>>=
to_ladok() {
  local course="$1"
  shift
  local modules="$@"

  for module in $modules; do
    local results=`fetch_results "$course" "$module"`
    cat "$results" \
      | <<remove Fs from the list>> \
      | <<rewrite course name to course code>> \
      | ladok report -fv
  done
}
@

To remove the Fs from the list we'll use the same pattern as in 
\cref{GeneratedFs}.
<<remove Fs from the list>>=
grep -v -E "\\s($FAILED_PATTERN)\\s"
@

For the tilkry24 course, the course name is \enquote{DD2520 VT24 (tilkry24)}.
So we'll rewrite this to \enquote{DD2520} before sending it to LADOK.
<<rewrite course name to course code>>=
sed -E "s/ ?[HV]T[0-9]{2}( \(.*\))?//"
@


\section{Throughput}

We can use the results in LADOK to estimate the throughput 
(\foreignlanguage{swedish}{genomströmning}) of the course.
Let's have a look at the kind of analysis that we'd like to do.

Let's have a look at the throughput of the course DD1310 for the academic year 
2024/2025.
We want the result to look like this:
\begin{pycode}
minted_output(['bash', 'results.sh', 'throughput',
               'DD1310',
               "-r", "50018", "51504", "50162", "50429", "50599"])
\end{pycode}
We see that the we have to use the round codes as in LADOK.
For those, we have the following mapping:
\begin{description}
\item[50018] TINTE
\item[51504] TIKED+CENMI
\item[50162] CELTE
\item[50429] CMAST1+CITEH2
\item[50599] CMATD+COPEN
\end{description}

Let's compare it to the throughput of the academic year 2023/2024.
Here we have the following mapping:
\begin{description}
\item[51804] CENMI+TIKED
\item[50077] CMAST1+CITEH2
\item[50374] CMATD+COPEN
\item[51364] CELTE
\end{description}
And the results:
\begin{pycode}
minted_output(['bash', 'results.sh', 'throughput',
               'DD1310',
               "-r", "51804", "50077", "50374", "51364"])
\end{pycode}

Let's also compare these results to the throughput of the course DD1317 for the 
years 2023 and 2024:
\begin{pycode}
minted_output(['bash', 'results.sh', 'throughput',
               'DD1317',
               "-r", "51345", "50566"])
\end{pycode}

We can also compare to the throughput of the master's level course DD2520 
Applied Cryptography for the years 2021--2025:
\begin{pycode}
minted_output(['bash', 'results.sh', 'throughput',
               'DD2520',
               "-r", "61703", "60716", "60475", "60992", "60549"])
\end{pycode}

Finally, let's add the results for the master's level course DA2215 Science of 
Cybersecurity.
This course is given twice a year, so throughput should be lower.
\begin{pycode}
minted_output(['bash', 'results.sh', 'throughput',
               'DA2215',
               "-r", "60444", "50709", "60362", "60364", "51289", "50398"])
\end{pycode}
We note that the first and second time it was offered, we had many second year 
students who needed to finish the course.
The remaining rounds were mostly first year students, so they had another year 
to finish it.

\subsection{Implementation overview}

The data we get from [[ladok]] looks like this:
\begin{minted}{text}
DD1310	51804	LAB1	student1	P	0.2119815668202765
DD1310	51804	LAB2	student1	P	0.3548387096774194
DD1310	51804	LAB3	student1	C	0.9078341013824884
DD1310	51804	DD1310	student1	C	0.9078341013824884
DD1310	51804	LAB1	student2	P	0.23963133640552994
DD1310	51804	LAB2	student2	P	0.3548387096774194
DD1310	51804	LAB3	student2	D	0.9216589861751152
DD1310	51804	DD1310	student2	D	0.9216589861751152
DD1310	51804	LAB1	student3	-	
DD1310	51804	LAB2	student3	P	0.3548387096774194
...
\end{minted}

We want to see how many students (percentage) passed each module and when.
Particularly, we want to know how many had passed by the end of the course and 
how many have passed now.

We can use the [[ladok]] command to get the results from LADOK.
The data above can be fetched with the command
\begin{minted}{bash}
ladok course DD1310 -r 51804 -ln
\end{minted}
Then we can use the same command, but with the option [[-t 1.0]], to get only 
the results by the end of the course.

We let the [[throughput]] command run these two commands to get the data.
The [[throughput]] function will take the course code as an argument.
Any other arguments are passed as is to the [[ladok course]] command.
That way we can also use filtering for course rounds.
<<functions>>=
throughput() {
  local course="$1"
  shift
  local args="$@"

  local results_all=$(ladok course "$course" -ln $args)
  if [[ $? -ne 0 ]]; then
    echo "Error fetching data from LADOK." >&2
    return 1
  fi

  local results_by_end=$(ladok course "$course" -t 1.0 -ln $args)
  if [[ $? -ne 0 ]]; then
    echo "Error fetching data from LADOK." >&2
    return 1
  fi

  <<output the throughput based on [[results_all]] and [[results_by_end]]>>
}
@

So we need two sets of data, each grouped by course and module.
So we'll write a function~[[throughput_stdin]] that takes the course data 
(output from [[ladok course]] above) on stdin and outputs the throughput for 
each module on stdout.
<<functions>>=
throughput_stdin() {
  local data=$(cat)

  local courses=$(echo "$data" | cut -f 1 | sort -u)
  local rounds=$(echo "$data" | cut -f 2 | sort -u)
  local modules=$(echo "$data" | cut -f 3 | sort -u)

  for course in $courses; do
    for round in $rounds; do
      for module in $modules; do
        local results=$(echo "$data" \
                        | grep -E "^$course\s$round\s$module\s")
        local passed=$(echo "$results" \
                       | grep -E "\s[A-EP]\s" | wc -l)
        local total=$(echo "$results" | wc -l)
        if [[ $total -gt 0 ]]; then
          local percentage=$(echo "scale=2; $passed / $total * 100" | bc)
          echo -e "$course $round $module:\t$percentage%"
        fi
      done
    done
  done
}
@

Since we want to compare the current throughput to that by the end of the 
course, we would like to print the results side by side.
We can use [[paste]] to do that.
<<output the throughput based on [[results_all]] and [[results_by_end]]>>=
paste <(echo "Current:                       "; \
        echo "$results_all" | throughput_stdin) \
      <(echo "At end:                        "; \
        echo "$results_by_end" | throughput_stdin)
@

\end{document}
